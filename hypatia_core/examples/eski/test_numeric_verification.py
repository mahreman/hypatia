import torch
import sys

print("======================================================================")
print("FEZ 9: PURE PYTORCH SAYISAL DOÄRULAMA (SANITY CHECK)")
print("======================================================================")
print("Hypatia'nÄ±n 'factor' optimizasyonunun (A*B+A*C -> A*(B+C))")
print("matematiksel geÃ§erliliÄŸini PURE TORCH ile test etme.")
print("NOT: 'torch.double' (float64) hassasiyeti kullanÄ±lÄ±yor.")

try:
    # --- BÃ–LÃœM 1: Ä°LERÄ° YAYILIM (FORWARD PASS) DOÄRULAMASI ---
    print("\n[TEST 1] Ä°leri YayÄ±lÄ±m (float64 / torch.double ile)")
    torch.manual_seed(0)
    
    # DÃœZELTME: dtype=torch.double eklendi
    A = torch.randn(7, 5, dtype=torch.double)
    B = torch.randn(5, 3, dtype=torch.double)
    C = torch.randn(5, 3, dtype=torch.double)

    # Orijinal, optimize edilmemiÅŸ yol
    lhs = A @ B + A @ C
    
    # Hypatia'nÄ±n bulduÄŸu optimize edilmiÅŸ yol
    rhs = A @ (B + C)
    
    # Hata miktarÄ±nÄ± gÃ¶relim
    diff = (lhs - rhs).abs().max()
    print(f"  Orijinal (lhs) vs Optimize (rhs) Max Hata: {diff.item()}")

    # Tolerans ile kontrol (daha hassas)
    is_allclose_forward = torch.allclose(lhs, rhs, atol=1e-10, rtol=1e-10)
    print(f"  SayÄ±sal EÅŸdeÄŸerlik (atol=1e-10): {is_allclose_forward}")
    
    assert is_allclose_forward, "Ä°leri yayÄ±lÄ±m, 'torch.double' ile bile sayÄ±sal olarak eÅŸdeÄŸer DEÄÄ°L!"
    print("  âœ… BAÅARILI: Ä°leri yayÄ±lÄ±m optimizasyonu 'torch.double' ile doÄŸrulanmÄ±ÅŸtÄ±r.")

    # --- BÃ–LÃœM 2: GERÄ° YAYILIM (GRADYAN) DOÄRULAMASI ---
    # (Bu bÃ¶lÃ¼m zaten sizin Ã¶nerinizde de torch.double kullanÄ±yordu)
    print("\n[TEST 2] Geri YayÄ±lÄ±m (torch.double ile)")
    A_grad = torch.randn(4, 6, dtype=torch.double, requires_grad=True)
    B_grad = torch.randn(6, 5, dtype=torch.double, requires_grad=True)
    C_grad = torch.randn(6, 5, dtype=torch.double, requires_grad=True)

    # Orijinal (Optimize EdilmemiÅŸ) Gradyanlar
    f1 = (A_grad @ B_grad + A_grad @ C_grad).sum()
    g1 = torch.autograd.grad(f1, (A_grad, B_grad, C_grad))
    
    # Optimize EdilmiÅŸ Gradyanlar
    f2 = (A_grad @ (B_grad + C_grad)).sum()
    g2 = torch.autograd.grad(f2, (A_grad, B_grad, C_grad))
    
    is_allclose_backward = all(torch.allclose(x, y, atol=1e-10, rtol=1e-10) for x, y in zip(g1, g2))
    print(f"  grad(f_orig) vs grad(f_opt) EÅŸdeÄŸerlik (atol=1e-10): {is_allclose_backward}")
    assert is_allclose_backward, "Geri yayÄ±lÄ±m gradyanlarÄ± 'torch.double' ile eÅŸdeÄŸer DEÄÄ°L!"
    print("  âœ… BAÅARILI: Geri yayÄ±lÄ±m optimizasyonu (AutoDiff) 'torch.double' ile doÄŸrulanmÄ±ÅŸtÄ±r.")

    print("\n" + "ğŸ†" * 20)
    print(" NÄ°HAÄ° KANIT BAÅARILI: Hypatia'nÄ±n temel optimizasyon")
    print(" varsayÄ±mÄ± (factor-out) hem ileri hem de geri yayÄ±lÄ±m")
    print(" iÃ§in 'torch.double' hassasiyetinde %100 doÄŸrulanmÄ±ÅŸtÄ±r.")
    print("ğŸ†" * 20)

except Exception as e:
    print(f"\n!!! HATA: {e}", file=sys.stderr)
    sys.exit(1)