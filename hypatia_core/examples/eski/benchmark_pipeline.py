import torch
import timeit
import sys
import hypatia_core as hc # Sadece HypatiaError'u yakalamak iÃ§in

print("=" * 70)
print("HYPATIA AI PIPELINE BENCHMARK (FEZ 9)")
print("=" * 70)
print("Hedef: Hypatia'nÄ±n v3.0 E-graph optimizasyonunun (Factoring)")
print("       saf PyTorch Ã¼zerinde ne kadar hÄ±zlanma saÄŸladÄ±ÄŸÄ±nÄ± Ã¶lÃ§mek.")

# --- CUDA KontrolÃ¼ ---
if not torch.cuda.is_available():
    print("\n!!! HATA: CUDA (GPU) DESTEÄžÄ° BULUNAMADI !!!", file=sys.stderr)
    print("Bu performans benchmark'Ä±, v1.0 hedefimiz olan GPU optimizasyonunu", file=sys.stderr)
    print("Ã¶lÃ§mek iÃ§in tasarlanmÄ±ÅŸtÄ±r. LÃ¼tfen CUDA destekli bir", file=sys.stderr)
    print("PyTorch kurulumu ile tekrar deneyin.", file=sys.stderr)
    sys.exit(1)

print(f"\nDonanÄ±m: {torch.cuda.get_device_name(0)}")

# ======================================================================
# BÃ–LÃœM 1: BENCHMARK PARAMETRELERÄ°
# ======================================================================

# BÃ¼yÃ¼k, gerÃ§ekÃ§i matris boyutlarÄ± kullanÄ±yoruz
N_BATCH = 4096      # Batch boyutu
D_IN = 1024         # Girdi boyutu
H_DIM = 512         # Gizli katman
D_OUT = 256         # Ã‡Ä±ktÄ± boyutu

# timeit iÃ§in Ã§alÄ±ÅŸtÄ±rma sayÄ±sÄ±
# (Daha hÄ±zlÄ± bir test iÃ§in 50'ye dÃ¼ÅŸÃ¼rebilirsiniz)
N_ITERATIONS = 100

print(f"Parametreler: Batch={N_BATCH}, Iterasyon={N_ITERATIONS}")
print(f"Senaryo: (X*W1)*W2 + (X*W1)*W3  vs  (X*W1)*(W2+W3)")

# ======================================================================
# BÃ–LÃœM 2: TENSORLERÄ°N HAZIRLANMASI (SETUP)
# ======================================================================

# timeit modÃ¼lÃ¼nÃ¼n kullanacaÄŸÄ± 'setup' kodu
# TÃ¼m tensÃ¶rleri GPU'ya gÃ¶nderiyoruz (.cuda())
SETUP_CODE = f"""
import torch

N, D_in, H, D_out = {N_BATCH}, {D_IN}, {H_DIM}, {D_OUT}
DTYPE = torch.float32 # Standart eÄŸitim hassasiyeti

# TensÃ¶rleri GPU Ã¼zerinde oluÅŸtur
x = torch.randn(N, D_in, device='cuda', dtype=DTYPE, requires_grad=True)
w1 = torch.randn(D_in, H, device='cuda', dtype=DTYPE, requires_grad=True)
w2 = torch.randn(H, D_out, device='cuda', dtype=DTYPE, requires_grad=True)
w3 = torch.randn(H, D_out, device='cuda', dtype=DTYPE, requires_grad=True)

# GradyanlarÄ± temizle (benchmark dÃ¶ngÃ¼sÃ¼ iÃ§inde tekrar yapÄ±lacak)
def zero_grads():
    x.grad = None
    w1.grad = None
    w2.grad = None
    w3.grad = None

# BaÅŸlamadan Ã¶nce her ÅŸeyin GPU'da hazÄ±r olduÄŸundan emin ol
torch.cuda.synchronize()
"""

# ======================================================================
# BÃ–LÃœM 3: Ã‡ALIÅžTIRILACAK Ä°FADELER
# ======================================================================

# Ä°fade 1: Orijinal (Optimize EdilmemiÅŸ) PyTorch Kodu
# (X*W1)*W2 + (X*W1)*W3
# Toplam 3 MatMul (ileri) + iliÅŸkili geri yayÄ±lÄ±m
STMT_ORIGINAL = """
zero_grads()

# Ä°leri yayÄ±lÄ±m (3 MatMul)
xw1 = x @ w1
o1 = xw1 @ w2
o2 = xw1 @ w3
y = o1 + o2

# Geri yayÄ±lÄ±m
y.sum().backward()

# KRÄ°TÄ°K: GPU'nun iÅŸi bitirmesini bekle
torch.cuda.synchronize()
"""

# Ä°fade 2: Hypatia'nÄ±n Optimize EttiÄŸi Kod
# (X*W1) * (W2+W3)
# Toplam 2 MatMul (ileri) + iliÅŸkili geri yayÄ±lÄ±m
STMT_OPTIMIZED = """
zero_grads()

# Ä°leri yayÄ±lÄ±m (2 MatMul + 1 Add)
xw1 = x @ w1
w2w3 = w2 + w3 # 'Add' iÅŸlemi MatMul'a gÃ¶re Ã§ok ucuzdur
y = xw1 @ w2w3

# Geri yayÄ±lÄ±m
y.sum().backward()

# KRÄ°TÄ°K: GPU'nun iÅŸi bitirmesini bekle
torch.cuda.synchronize()
"""

# ======================================================================
# BÃ–LÃœM 4: BENCHMARK'I Ã‡ALIÅžTIR
# ======================================================================

try:
    print(f"\n[TEST 1] Orijinal (Optimize EdilmemiÅŸ) Kod Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
    print(f"({N_ITERATIONS} iterasyon, 3 MatMul/iterasyon)")
    
    time_orig = timeit.timeit(
        stmt=STMT_ORIGINAL,
        setup=SETUP_CODE,
        number=N_ITERATIONS
    )
    avg_orig = (time_orig / N_ITERATIONS) * 1000 # Saniyeden milisaniyeye Ã§evir
    
    print(f"  Toplam SÃ¼re: {time_orig:.4f} saniye")
    print(f"  Ortalama:    {avg_orig:.4f} ms / iterasyon")


    print(f"\n[TEST 2] Hypatia (Optimize EdilmiÅŸ) Kod Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
    print(f"({N_ITERATIONS} iterasyon, 2 MatMul/iterasyon)")
    
    time_opt = timeit.timeit(
        stmt=STMT_OPTIMIZED,
        setup=SETUP_CODE,
        number=N_ITERATIONS
    )
    avg_opt = (time_opt / N_ITERATIONS) * 1000 # Saniyeden milisaniyeye Ã§evir
    
    print(f"  Toplam SÃ¼re: {time_opt:.4f} saniye")
    print(f"  Ortalama:    {avg_opt:.4f} ms / iterasyon")

    # ======================================================================
    # BÃ–LÃœM 5: SONUÃ‡LAR
    # ======================================================================
    
    print("\n" + "=" * 70)
    print(" BENCHMARK SONUÃ‡LARI (FEZ 9)")
    print("=" * 70)
    
    if time_opt < time_orig:
        speedup = (time_orig / time_opt)
        percentage = (1.0 - (time_opt / time_orig)) * 100
        print(f"  ðŸ† BAÅžARILI: Optimize edilmiÅŸ kod {speedup:.2f}x daha hÄ±zlÄ±!")
        print(f"  Optimize EdilmemiÅŸ Ortalama: {avg_orig:.4f} ms")
        print(f"  Optimize EdilmiÅŸ Ortalama:   {avg_opt:.4f} ms")
        print(f"  KAZANÃ‡: %{percentage:.2f} hÄ±zlanma")
    else:
        print(f"  âš ï¸ BAÅžARISIZ: Optimizasyon kodu yavaÅŸlattÄ±.")
        print(f"  Optimize EdilmemiÅŸ Ortalama: {avg_orig:.4f} ms")
        print(f"  Optimize EdilmiÅŸ Ortalama:   {avg_opt:.4f} ms")

    print("\n" + "âœ…" * 20)
    print(" BAÅžARILI: FEZ 9 TAMAMLANDI!")
    print("v1.0 MVP'nin (FLOPs optimizasyonu) somut etkisi Ã¶lÃ§Ã¼ldÃ¼.")
    print("âœ…" * 20)

except Exception as e:
    print(f"\n!!! BENCHMARK HATASI: {e}", file=sys.stderr)
    sys.exit(1)